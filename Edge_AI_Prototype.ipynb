{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "eB3dO_tWyYFP",
        "outputId": "e89c331c-f466-420b-b2ec-4a2ac5d62732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Downloading Dataset ---\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3810480125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Download the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Define the data directory path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/opendatasets/__init__.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Check for a Kaggle dataset URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_kaggle_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdownload_kaggle_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Check for Google Drive URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/opendatasets/utils/kaggle_api.py\u001b[0m in \u001b[0;36mdownload_kaggle_dataset\u001b[0;34m(dataset_url, data_dir, force, dry_run)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mread_kaggle_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your Kaggle username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_kaggle_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Part 2, Task 1: Edge AI Prototype (Recyclable Item Classifier)\n",
        "#\n",
        "# This script will:\n",
        "# 1. Set up the Colab environment and download the TrashNet dataset.\n",
        "# 2. Preprocess the data and set up training/validation sets.\n",
        "# 3. Build and train a lightweight MobileNetV2 model.\n",
        "# 4. Evaluate the Keras model.\n",
        "# 5. Convert the Keras model to a quantized TensorFlow Lite model.\n",
        "# 6. Test the TFLite model's performance and accuracy in Colab.\n",
        "# 7. Provide a sample deployment script for a Raspberry Pi.\n",
        "# ==============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "\n",
        "# === STEP 1: DOWNLOAD AND PREPARE THE DATASET ===\n",
        "\n",
        "print(\"--- Step 1: Downloading Dataset ---\")\n",
        "\n",
        "# We will use 'opendatasets' to easily fetch the Kaggle dataset\n",
        "# You will be prompted to enter your Kaggle username and API key\n",
        "!pip install -q opendatasets\n",
        "\n",
        "import opendatasets as od\n",
        "\n",
        "# Dataset URL from Kaggle\n",
        "dataset_url = 'https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification'\n",
        "\n",
        "# Download the dataset\n",
        "od.download(dataset_url)\n",
        "\n",
        "# Define the data directory path\n",
        "data_dir = pathlib.Path('./garbage-classification/Garbage classification')\n",
        "\n",
        "# Verify the download\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(f\"\\nSuccessfully downloaded {image_count} images.\")\n",
        "\n",
        "# === STEP 2: LOAD AND PREPROCESS DATA ===\n",
        "\n",
        "print(\"\\n--- Step 2: Loading and Preprocessing Data ---\")\n",
        "\n",
        "# Define parameters for the data loader\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "# Create the training dataset (80% of data)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=IMG_SIZE,\n",
        "  batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create the validation dataset (20% of data)\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=IMG_SIZE,\n",
        "  batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Get the class names\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Configure the dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# === STEP 3: BUILD THE MODEL WITH TRANSFER LEARNING ===\n",
        "\n",
        "print(\"\\n--- Step 3: Building Model (MobileNetV2) ---\")\n",
        "\n",
        "# Define a data augmentation layer to prevent overfitting\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\", input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Load the MobileNetV2 base model (pre-trained on ImageNet)\n",
        "# include_top=False means we don't load the final classification layer\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "\n",
        "# Freeze the base model so we only train our new layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the new model on top\n",
        "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "x = data_augmentation(inputs) # Apply augmentation\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(x) # Preprocess for MobileNetV2\n",
        "x = base_model(x, training=False) # Run the base model\n",
        "x = layers.GlobalAveragePooling2D()(x) # Pool the features\n",
        "x = layers.Dropout(0.2)(x) # Add dropout for regularization\n",
        "outputs = layers.Dense(len(class_names))(x) # Add our new classification layer\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# === STEP 4: TRAIN THE MODEL ===\n",
        "\n",
        "print(\"\\n--- Step 4: Training the Model ---\")\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# Evaluate the final Keras model\n",
        "print(\"\\nEvaluating final Keras model...\")\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "print(f\"Final Keras Model Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Save the Keras model\n",
        "model.save('recyclable_model.h5')\n",
        "print(\"Keras model saved as 'recyclable_model.h5'\")\n",
        "\n",
        "# === STEP 5: CONVERT TO TENSORFLOW LITE (TFLITE) ===\n",
        "\n",
        "print(\"\\n--- Step 5: Converting to TensorFlow Lite (Quantized) ---\")\n",
        "\n",
        "# Initialize the TFLiteConverter from the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Apply default optimizations (this includes quantization)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model_quant = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open('model_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model_quant)\n",
        "\n",
        "print(\"Quantized TFLite model saved as 'model_quant.tflite'\")\n",
        "\n",
        "# Compare file sizes\n",
        "keras_size = os.path.getsize('recyclable_model.h5') / (1024 * 1024)\n",
        "tflite_size = os.path.getsize('model_quant.tflite') / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nKeras Model Size:    {keras_size:.2f} MB\")\n",
        "print(f\"TFLite Model Size: {tflite_size:.2f} MB\")\n",
        "print(f\"TFLite model is {keras_size/tflite_size:.1f}x smaller!\")\n",
        "\n",
        "# === STEP 6: TEST THE TFLITE MODEL (SIMULATION) ===\n",
        "\n",
        "print(\"\\n--- Step 6: Testing TFLite Model Performance ---\")\n",
        "\n",
        "# Load the TFLite model with the Interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path='model_quant.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# --- Test inference time ---\n",
        "# We run a dummy inference to \"warm up\" the interpreter\n",
        "interpreter.set_tensor(input_details[0]['index'], np.zeros((1, 224, 224, 3), dtype=np.float32))\n",
        "interpreter.invoke()\n",
        "\n",
        "# Now, time the inference\n",
        "test_image = np.zeros((1, 224, 224, 3), dtype=np.float32)\n",
        "start_time = time.time()\n",
        "interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "interpreter.invoke()\n",
        "end_time = time.time()\n",
        "print(f\"TFLite Inference Time (Colab CPU): {(end_time - start_time) * 1000:.2f} ms\")\n",
        "\n",
        "# --- Test accuracy on a batch from the validation set ---\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "\n",
        "for images, labels in val_ds.take(5): # Test on 5 batches\n",
        "  for i in range(len(labels)):\n",
        "    total_images += 1\n",
        "    # Get a single image and expand dimensions\n",
        "    img = images[i]\n",
        "    img = tf.expand_dims(img, 0) # Add batch dimension\n",
        "\n",
        "    # Set the tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], img)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    prediction = np.argmax(output_data)\n",
        "\n",
        "    if prediction == labels[i]:\n",
        "      correct_predictions += 1\n",
        "\n",
        "tflite_accuracy = correct_predictions / total_images\n",
        "print(f\"TFLite Model Accuracy (on {total_images} images): {tflite_accuracy:.2%}\")\n",
        "\n",
        "print(\"\\n=== TASK 1 COMPLETE ===\")\n",
        "\n",
        "# === STEP 7: SAMPLE DEPLOYMENT SCRIPT (FOR RASPBERRY PI) ===\n",
        "\n",
        "# The following code is NOT for Colab. It is what you would\n",
        "# save as a 'classify.py' file on your Raspberry Pi.\n",
        "\n",
        "pi_script = \"\"\"\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# --- CONFIG ---\n",
        "MODEL_PATH = \"model_quant.tflite\"\n",
        "IMAGE_PATH = sys.argv[1] # Get image path from command line\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "CLASS_NAMES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "# --- END CONFIG ---\n",
        "\n",
        "def load_model(model_path):\n",
        "    \\\"\\\"\\\"Loads the TFLite model and allocates tensors.\\\"\\\"\\\"\n",
        "    interpreter = tflite.Interpreter(model_path=model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    return interpreter\n",
        "\n",
        "def preprocess_image(image_path, img_size):\n",
        "    \\\"\\\"\\\"Loads and preprocesses an image for the model.\\\"\\\"\\\"\n",
        "    img = Image.open(image_path).resize(img_size)\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    # Add a batch dimension\n",
        "    input_data = np.expand_dims(img, axis=0)\n",
        "    return input_data\n",
        "\n",
        "def run_inference(interpreter, input_data):\n",
        "    \\\"\\\"\\\"Runs inference on the preprocessed image.\\\"\\\"\\\"\n",
        "    # Get I/O details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Set the input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return output_data\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) != 2:\n",
        "        print(\"Usage: python classify.py <image_path>\")\n",
        "        return\n",
        "\n",
        "    print(\"Loading TFLite model...\")\n",
        "    interpreter = load_model(MODEL_PATH)\n",
        "\n",
        "    print(f\"Processing image: {IMAGE_PATH}\")\n",
        "    input_data = preprocess_image(IMAGE_PATH, (IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    raw_prediction = run_inference(interpreter, input_data)\n",
        "\n",
        "    # Post-process the result\n",
        "    predicted_index = np.argmax(raw_prediction)\n",
        "    predicted_class = CLASS_NAMES[predicted_index]\n",
        "    confidence = np.max(raw_prediction) # Note: this is raw logit, not probability\n",
        "\n",
        "    print(\"--- RESULT ---\")\n",
        "    print(f\"Predicted Class: {predicted_class}\")\n",
        "    print(\"--------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Sample Raspberry Pi Deployment Script (classify.py) ---\")\n",
        "print(pi_script)"
      ]
    }
  ]
}